{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 9 XOR\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer1\"):\n",
    "    W1 = tf.Variable(tf.random_uniform([2, 2]), name=\"weight_1\")\n",
    "    b1 = tf.Variable(tf.random_uniform([2]), name=\"bias_1\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    tf.summary.histogram(\"W1\", W1)\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"Layer1\", layer1)\n",
    "\n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight_2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name=\"bias_2\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    tf.summary.histogram(\"W2\", W2)\n",
    "    tf.summary.histogram(\"b2\", b2)\n",
    "    tf.summary.histogram(\"Hypothesis\", hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "with tf.name_scope(\"Cost\"):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    tf.summary.scalar(\"Cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6958813 weight: [[0.56555724 0.14830016]\n",
      " [0.4450029  0.7209047 ]]\n",
      "100 0.6661593 weight: [[0.27992743 1.3479745 ]\n",
      " [0.2067321  1.9930536 ]]\n",
      "200 0.53621197 weight: [[0.7624368 2.8622847]\n",
      " [1.4767566 3.6350594]]\n",
      "300 0.29072398 weight: [[2.3691506 4.0904403]\n",
      " [2.483441  4.801646 ]]\n",
      "400 0.15460843 weight: [[3.2243977 4.848526 ]\n",
      " [3.3224382 5.5280056]]\n",
      "500 0.09570372 weight: [[3.747631  5.3313475]\n",
      " [3.8394246 5.99529  ]]\n",
      "600 0.06595826 weight: [[4.1107764 5.674253 ]\n",
      " [4.198565  6.328675 ]]\n",
      "700 0.048723143 weight: [[4.385569  5.9372616]\n",
      " [4.4703355 6.585178 ]]\n",
      "800 0.037728205 weight: [[4.605516  6.149593 ]\n",
      " [4.6878343 6.792762 ]]\n",
      "900 0.030218298 weight: [[4.788465  6.327252 ]\n",
      " [4.8687263 6.966791 ]]\n",
      "1000 0.024823656 weight: [[4.944924  6.479832 ]\n",
      " [5.023406  7.1165056]]\n",
      "1100 0.020796275 weight: [[5.081559  6.613506 ]\n",
      " [5.1584797 7.2478547]]\n",
      "1200 0.017696973 weight: [[5.2028484 6.732458 ]\n",
      " [5.2783766 7.3648868]]\n",
      "1300 0.01525273 weight: [[5.311932  6.839653 ]\n",
      " [5.3862042 7.4704704]]\n",
      "1400 0.013285928 weight: [[5.411098  6.9372582]\n",
      " [5.4842257 7.5666986]]\n",
      "1500 0.011676347 weight: [[5.5020585 7.026908 ]\n",
      " [5.574138  7.6551585]]\n",
      "1600 0.01034001 weight: [[5.5861278 7.1098576]\n",
      " [5.6572366 7.7370744]]\n",
      "1700 0.009216827 weight: [[5.664335  7.187099 ]\n",
      " [5.7345433 7.8134065]]\n",
      "1800 0.008262675 weight: [[5.7375035 7.259418 ]\n",
      " [5.806872  7.884923 ]]\n",
      "1900 0.0074444483 weight: [[5.806294  7.3274612]\n",
      " [5.8748713 7.952254 ]]\n",
      "2000 0.006737 weight: [[5.8712544 7.391756 ]\n",
      " [5.9390864 8.015909 ]]\n",
      "2100 0.0061208294 weight: [[5.9328303 7.4527397]\n",
      " [5.99996   8.076321 ]]\n",
      "2200 0.0055805435 weight: [[5.991408  7.510781 ]\n",
      " [6.0578694 8.1338415]]\n",
      "2300 0.0051040677 weight: [[6.0473075 7.566193 ]\n",
      " [6.113132  8.188786 ]]\n",
      "2400 0.0046816394 weight: [[6.1008    7.619241 ]\n",
      " [6.1660185 8.241406 ]]\n",
      "2500 0.004305243 weight: [[6.1521215 7.6701565]\n",
      " [6.2167616 8.29193  ]]\n",
      "2600 0.0039684237 weight: [[6.2014775 7.7191367]\n",
      " [6.2655616 8.340553 ]]\n",
      "2700 0.0036657874 weight: [[6.2490454 7.7663574]\n",
      " [6.312595  8.387445 ]]\n",
      "2800 0.0033928768 weight: [[6.29498   7.8119683]\n",
      " [6.3580136 8.432757 ]]\n",
      "2900 0.003145976 weight: [[6.339417  7.8561063]\n",
      " [6.4019566 8.476617 ]]\n",
      "3000 0.0029218374 weight: [[6.3824806 7.898889 ]\n",
      " [6.444541  8.519144 ]]\n",
      "3100 0.0027177855 weight: [[6.424278  7.9404235]\n",
      " [6.4858747 8.56044  ]]\n",
      "3200 0.002531583 weight: [[6.464903  7.9808   ]\n",
      " [6.5260506 8.600599 ]]\n",
      "3300 0.0023611577 weight: [[6.504441 8.020105]\n",
      " [6.565154 8.639703]]\n",
      "3400 0.0022049192 weight: [[6.542971  8.058416 ]\n",
      " [6.6032624 8.677825 ]]\n",
      "3500 0.0020612928 weight: [[6.5805616 8.095802 ]\n",
      " [6.6404433 8.715036 ]]\n",
      "3600 0.0019290487 weight: [[6.6172795 8.132322 ]\n",
      " [6.6767583 8.751394 ]]\n",
      "3700 0.001807048 weight: [[6.6531773 8.168033 ]\n",
      " [6.712265  8.786957 ]]\n",
      "3800 0.0016942717 weight: [[6.688309  8.202985 ]\n",
      " [6.7470136 8.821774 ]]\n",
      "3900 0.001589911 weight: [[6.722721  8.23723  ]\n",
      " [6.7810526 8.855888 ]]\n",
      "4000 0.0014931273 weight: [[6.756456  8.270804 ]\n",
      " [6.8144255 8.889347 ]]\n",
      "4100 0.0014032619 weight: [[6.7895546 8.303751 ]\n",
      " [6.84717   8.922182 ]]\n",
      "4200 0.0013197312 weight: [[6.8220577 8.336104 ]\n",
      " [6.8793235 8.9544325]]\n",
      "4300 0.0012419664 weight: [[6.8539896 8.367898 ]\n",
      " [6.910916  8.986133 ]]\n",
      "4400 0.0011695188 weight: [[6.8853874 8.3991585]\n",
      " [6.94198   9.017307 ]]\n",
      "4500 0.0011018948 weight: [[6.9162755 8.429918 ]\n",
      " [6.9725432 9.047988 ]]\n",
      "4600 0.001038795 weight: [[6.9466887 8.460203 ]\n",
      " [7.00263   9.078201 ]]\n",
      "4700 0.0009797415 weight: [[6.9766417 8.490039 ]\n",
      " [7.032269  9.1079645]]\n",
      "4800 0.0009245097 weight: [[7.0061603 8.519439 ]\n",
      " [7.0614786 9.137309 ]]\n",
      "4900 0.0008728008 weight: [[7.0352645 8.548433 ]\n",
      " [7.090281  9.1662445]]\n",
      "5000 0.000824286 weight: [[7.063982  8.577038 ]\n",
      " [7.118694  9.1947975]]\n",
      "5100 0.00077883067 weight: [[7.092315 8.605271]\n",
      " [7.146733 9.222982]]\n",
      "5200 0.0007361362 weight: [[7.120291  8.633152 ]\n",
      " [7.1744227 9.250815 ]]\n",
      "5300 0.00069600827 weight: [[7.1479263 8.660686 ]\n",
      " [7.2017736 9.2783165]]\n",
      "5400 0.00065829756 weight: [[7.1752377 8.687899 ]\n",
      " [7.2287993 9.305494 ]]\n",
      "5500 0.00062283984 weight: [[7.20223  8.714802]\n",
      " [7.255514 9.332364]]\n",
      "5600 0.0005894558 weight: [[7.228912 8.741404]\n",
      " [7.281933 9.358939]]\n",
      "5700 0.0005580112 weight: [[7.2553167 8.767718 ]\n",
      " [7.3080626 9.385231 ]]\n",
      "5800 0.0005284013 weight: [[7.281435  8.7937565]\n",
      " [7.3339205 9.411253 ]]\n",
      "5900 0.00050044723 weight: [[7.3072944 8.819529 ]\n",
      " [7.359513  9.437009 ]]\n",
      "6000 0.00047411886 weight: [[7.332887 8.84505 ]\n",
      " [7.384858 9.462513]]\n",
      "6100 0.0004492671 weight: [[7.35825   8.870321 ]\n",
      " [7.4099374 9.487778 ]]\n",
      "6200 0.00042581727 weight: [[7.3833475 8.895359 ]\n",
      " [7.4348164 9.512804 ]]\n",
      "6300 0.00040365 weight: [[7.4082246 8.920164 ]\n",
      " [7.4594526 9.537604 ]]\n",
      "6400 0.0003827652 weight: [[7.4328976 8.944751 ]\n",
      " [7.483834  9.562192 ]]\n",
      "6500 0.00036298396 weight: [[7.4573307 8.969127 ]\n",
      " [7.50804   9.586568 ]]\n",
      "6600 0.00034427637 weight: [[7.481557 8.99329 ]\n",
      " [7.532038 9.610734]]\n",
      "6700 0.00032662746 weight: [[7.505593  9.017253 ]\n",
      " [7.5558295 9.634703 ]]\n",
      "6800 0.000309903 weight: [[7.529425 9.041024]\n",
      " [7.579421 9.658483]]\n",
      "6900 0.00029407316 weight: [[7.5530877 9.064608 ]\n",
      " [7.6028013 9.682078 ]]\n",
      "7000 0.00027907826 weight: [[7.576537 9.088011]\n",
      " [7.626046 9.70549 ]]\n",
      "7100 0.00026491832 weight: [[7.5998187 9.111233 ]\n",
      " [7.64911   9.7287245]]\n",
      "7200 0.00025150378 weight: [[7.622919  9.134279 ]\n",
      " [7.6719956 9.751787 ]]\n",
      "7300 0.00023874524 weight: [[7.645852  9.157161 ]\n",
      " [7.6947303 9.774686 ]]\n",
      "7400 0.0002267172 weight: [[7.6686387 9.179883 ]\n",
      " [7.7172685 9.797425 ]]\n",
      "7500 0.00021533018 weight: [[7.691248  9.20244  ]\n",
      " [7.7396584 9.820001 ]]\n",
      "7600 0.00020446494 weight: [[7.713708  9.224843 ]\n",
      " [7.7619023 9.842432 ]]\n",
      "7700 0.00019419598 weight: [[7.736011 9.247102]\n",
      " [7.783997 9.864706]]\n",
      "7800 0.00018447857 weight: [[7.7581606 9.269203 ]\n",
      " [7.8059354 9.886841 ]]\n",
      "7900 0.0001752829 weight: [[7.780185 9.291172]\n",
      " [7.827694 9.908832]]\n",
      "8000 0.00016654932 weight: [[7.8020754 9.31299  ]\n",
      " [7.849342  9.930692 ]]\n",
      "8100 0.00015823312 weight: [[7.8238077 9.334681 ]\n",
      " [7.8708863 9.952405 ]]\n",
      "8200 0.00015034917 weight: [[7.8454175 9.3562355]\n",
      " [7.892308  9.973994 ]]\n",
      "8300 0.00014286769 weight: [[7.866862 9.377655]\n",
      " [7.91357  9.995449]]\n",
      "8400 0.00013577375 weight: [[ 7.888206   9.398954 ]\n",
      " [ 7.9347224 10.016774 ]]\n",
      "8500 0.00012903752 weight: [[ 7.909408   9.420122 ]\n",
      " [ 7.9557266 10.037981 ]]\n",
      "8600 0.00012264411 weight: [[ 7.9305096  9.441163 ]\n",
      " [ 7.9766364 10.059062 ]]\n",
      "8700 0.00011657861 weight: [[ 7.9514523  9.462089 ]\n",
      " [ 7.9973855 10.08002  ]]\n",
      "8800 0.00011081121 weight: [[ 7.9723067  9.4829035]\n",
      " [ 8.018057  10.1008625]]\n",
      "8900 0.00010534188 weight: [[ 7.9930477  9.503589 ]\n",
      " [ 8.038599  10.121594 ]]\n",
      "9000 0.00010015575 weight: [[ 8.013654  9.524161]\n",
      " [ 8.059001 10.142215]]\n",
      "9100 9.5222975e-05 weight: [[ 8.034164   9.544625 ]\n",
      " [ 8.079324  10.1627245]]\n",
      "9200 9.052867e-05 weight: [[ 8.05457   9.564981]\n",
      " [ 8.099539 10.183117]]\n",
      "9300 8.608773e-05 weight: [[ 8.074825  9.58523 ]\n",
      " [ 8.119619 10.203403]]\n",
      "9400 8.185544e-05 weight: [[ 8.095004  9.605373]\n",
      " [ 8.139612 10.223584]]\n",
      "9500 7.7816905e-05 weight: [[ 8.115107  9.625415]\n",
      " [ 8.159499 10.243664]]\n",
      "9600 7.403173e-05 weight: [[ 8.135077  9.645346]\n",
      " [ 8.179299 10.263644]]\n",
      "9700 7.038067e-05 weight: [[ 8.154932   9.665176 ]\n",
      " [ 8.198969  10.2835245]]\n",
      "9800 6.693827e-05 weight: [[ 8.174677  9.684909]\n",
      " [ 8.218544 10.303305]]\n",
      "9900 6.36449e-05 weight: [[ 8.194344  9.704542]\n",
      " [ 8.238035 10.322988]]\n",
      "10000 6.054526e-05 weight: [[ 8.213952  9.724079]\n",
      " [ 8.257421 10.342579]]\n",
      "\n",
      "Hypothesis:\n",
      "[[6.7620225e-05]\n",
      " [9.9994206e-01]\n",
      " [9.9993920e-01]\n",
      " [5.5643919e-05]] \n",
      "Predicted:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, summary, cost_val, w_val = sess.run(\n",
    "            [train, merged_summary, cost, W1], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val, \"weight:\", w_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "\n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHypothesis:\\n[[6.1310326e-05]\\n [9.9993694e-01]\\n [9.9995077e-01]\\n [5.9751470e-05]] \\nPredicted:\\n[[0.]\\n [1.]\\n [1.]\\n [0.]] \\nAccuracy:\\n1.0\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hypothesis:\n",
    "[[6.1310326e-05]\n",
    " [9.9993694e-01]\n",
    " [9.9995077e-01]\n",
    " [5.9751470e-05]] \n",
    "Predicted:\n",
    "[[0.]\n",
    " [1.]\n",
    " [1.]\n",
    " [0.]] \n",
    "Accuracy:\n",
    "1.0\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
